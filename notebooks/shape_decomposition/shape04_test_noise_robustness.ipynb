{
 "cells": [
  {
   "cell_type": "code",
   "id": "bcc4fa71-822e-4ef4-afbb-72021eb48987",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import sys\n",
    "sys.path.append('/home/nick/projects')\n",
    "# from PointGPT import segmentation\n",
    "from fle_3d.fle_3d import FLEBasis3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob2 import glob\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from src.utilities.functions import path_leaf\n",
    "from src.utilities.fin_class_def import FinData\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KernelDensity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "955a418c-cb7e-47d9-af93-b412317a3137",
   "metadata": {},
   "source": [
    "### Notebook to explore the use of spherical harmonics to read-out fin shape\n",
    "As a first pass, I will perform warps on a single fin dataset, so that I have a solid sense for what to expect. \n",
    "\n",
    "I also hope to test whether we can generate an \"average\" fin. In this case, I will construct the warps such that the average is just the original fin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbeff78-f785-47a3-9c4e-70c8835922b0",
   "metadata": {},
   "source": [
    "### Load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "90abfbdb-9742-400f-a1b5-27581bdef403",
   "metadata": {},
   "source": [
    "root = \"/media/nick/hdd02/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/pecfin_dynamics/\"\n",
    "\n",
    "fin_object_path = os.path.join(root, \"point_cloud_data\", \"fin_objects\", \"\")\n",
    "fin_object_list = sorted(glob(fin_object_path + \"*.pkl\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81865799-d315-4f53-82cd-0b6b0bcb8a41",
   "metadata": {},
   "source": [
    "file_ind01 = 46\n",
    "seg_type = \"tissue_only_best_model_tissue\"\n",
    "\n",
    "fp01 = fin_object_list[file_ind01]\n",
    "point_prefix01 = path_leaf(fp01).replace(\"_fin_object.pkl\", \"\")\n",
    "print(point_prefix01)\n",
    "\n",
    "fin_data = FinData(data_root=root, name=point_prefix01, tissue_seg_model=seg_type)\n",
    "fin_df = fin_data.full_point_data\n",
    "fin_df = fin_df.loc[fin_df[\"fin_label_curr\"]==1, :]\n",
    "fin_points = fin_df[[\"X\", \"Y\", \"Z\"]].to_numpy()\n",
    "\n",
    "fin_axis_df = fin_data.axis_fin\n",
    "fin_axes = fin_data.calculate_axis_array(fin_axis_df)\n",
    "fin_points_pca = np.matmul(fin_points - np.mean(fin_points, axis=0), fin_axes.T)\n",
    "fin_df.loc[:, [\"XP\", \"YP\", \"ZP\"]] = fin_points_pca"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f24ec4fd-6278-40cc-a882-054d280b44da",
   "metadata": {},
   "source": [
    "### Apply simple warps"
   ]
  },
  {
   "cell_type": "code",
   "id": "d9911056-27a8-43c0-bb45-f6be3087a167",
   "metadata": {},
   "source": [
    "def apply_random_warps(points, mean_log=0, sigma_log=0.1):\n",
    "    \"\"\"\n",
    "    Apply random warps to a 3D point cloud using a multivariate lognormal distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    points (numpy.ndarray): Nx3 array of points.\n",
    "    mean_log (float): Mean of the lognormal distribution (default is 0, which corresponds to warps centered around 1).\n",
    "    sigma_log (float): Standard deviation of the lognormal distribution (controls the variability of the warps).\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Warped point cloud.\n",
    "    \"\"\"\n",
    "    # Number of points in the point cloud\n",
    "    N, dim = points.shape\n",
    "    \n",
    "    if dim != 3:\n",
    "        raise ValueError(\"Input points should be an Nx3 numpy array representing a 3D point cloud.\")\n",
    "    \n",
    "    # Generate random scaling factors using a lognormal distribution centered around 1\n",
    "    scale_factors = np.random.lognormal(mean=mean_log, sigma=sigma_log, size=(1, 3))\n",
    "    \n",
    "    # Apply the scaling factors to the point cloud\n",
    "    warped_points = np.multiply(points,  scale_factors)\n",
    "    \n",
    "    return warped_points, scale_factors"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "694c4dc1-0562-4173-bbe8-adecdec30261",
   "metadata": {},
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# test out the warp function\n",
    "raw_points = fin_df[[\"ZP\", \"YP\", \"XP\"]].to_numpy() # note that Ii permute the axes for convenience\n",
    "\n",
    "# Apply random warps to the point cloud\n",
    "warped_points, warp = apply_random_warps(raw_points, sigma_log=.5)\n",
    "print(warp)\n",
    "fig = go.Figure() #make_subplots(rows=1, cols=2, specs=[[{'type': 'scatter3d'}, {'type': 'scatter3d'}]])\n",
    "\n",
    "\n",
    "# Add the first plot to the first subplot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(x=raw_points[:, 0], y=raw_points[:, 1], z=raw_points[:, 2], mode='markers')),\n",
    "    # row=1, col=1\n",
    "# )\n",
    "\n",
    "# Add the second plot to the second subplot\n",
    "fig.add_trace(\n",
    "    go.Scatter3d(x=warped_points[:, 0], y=warped_points[:, 1], z=warped_points[:, 2], mode='markers')),\n",
    "#     row=1, col=2\n",
    "# )\n",
    "\n",
    "fig.update_layout(\n",
    "             scene=dict(\n",
    "                 aspectmode='data'\n",
    "                 ))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fbb86af8-141e-486f-8363-6fb5bb23a5a4",
   "metadata": {},
   "source": [
    "### Generat synthetic dataset of warped fin-variants"
   ]
  },
  {
   "cell_type": "code",
   "id": "65fc95da-0ff8-46a9-b0f5-0e69dd8eca58",
   "metadata": {},
   "source": [
    "warp_sigma = 0.1 # log sigma for lognormal warp distribution\n",
    "n_samples = 100 # number of synthetic fins to generate\n",
    "\n",
    "# test out the warp function\n",
    "raw_points = fin_df[[\"ZP\", \"YP\", \"XP\"]].to_numpy()\n",
    "\n",
    "warp_list = []\n",
    "warped_points_list = []\n",
    "np.random.seed(61)\n",
    "max_dim = int(np.ceil(np.max(np.abs(raw_points)) / 5) * 5)\n",
    "\n",
    "for n in tqdm(range(n_samples)):\n",
    "    # Apply random warps to the point cloud\t\n",
    "    warped_points, warp = apply_random_warps(raw_points, sigma_log=warp_sigma)\n",
    "\n",
    "    # save \n",
    "    warp_list.append(warp)\n",
    "    warped_points_list.append(warped_points)\n",
    "\n",
    "    max_dim = np.max([int(np.ceil(np.max(np.abs(warped_points)) / 5) * 5), max_dim])\n",
    "\n",
    "warp_df = pd.DataFrame(np.squeeze(np.asarray(warp_list)), columns=[\"x_warp\", \"y_warp\", \"z_warp\"])\n",
    "\n",
    "# set first entry to be origina\n",
    "warp_df.loc[0, :] = 1\n",
    "warped_points_list[0] = raw_points\n",
    "print(max_dim)\n",
    "warp_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8cb86886-6663-4121-87f9-9ba9cac9f242",
   "metadata": {},
   "source": [
    "#############\n",
    "# calculate density-based representations for each point cloud\n",
    "kde_bw = 4\n",
    "res = 5 # in um\n",
    "N = int(np.ceil(2*max_dim / res)) + 1\n",
    "\n",
    "# make ref grids\n",
    "x_axis = np.linspace(-max_dim, max_dim, N)\n",
    "y_axis = np.linspace(-max_dim, max_dim, N)\n",
    "z_axis = np.linspace(-max_dim, max_dim, N)\n",
    "x_grid, y_grid, z_grid = np.meshgrid(x_axis, y_axis, z_axis)\n",
    "xyz_array = np.c_[x_grid.ravel(), y_grid.ravel(), z_grid.ravel()]\n",
    "\n",
    "# Go even simpler and just calculate 3D histogram\n",
    "x_bins = np.linspace(-max_dim, max_dim, N+1)\n",
    "y_bins = np.linspace(-max_dim, max_dim, N+1)\n",
    "z_bins = np.linspace(-max_dim, max_dim, N+1)\n",
    "\n",
    "\n",
    "density_list = []\n",
    "\n",
    "for i in tqdm(range(n_samples)):\n",
    "    points = warped_points_list[i]\n",
    "    kde = KernelDensity(bandwidth=kde_bw, kernel=\"gaussian\").fit(points) \n",
    "\n",
    "    probs = np.exp(kde.score_samples(xyz_array))\n",
    "    # counts, _ = np.histogramdd(points, (x_bins, y_bins, z_bins))\n",
    "    probs = probs / np.max(probs)\n",
    "\n",
    "    density_list.append(probs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2cf05b60-8e0a-4ddc-a6c5-0d1e86cd45b9",
   "metadata": {},
   "source": [
    "The idea is to use these warped fins as a point of reference for assessing the size of noise impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a011e29-4e37-4092-b2dd-2395abaca864",
   "metadata": {},
   "source": [
    "#### Let's check out a sample dataset just to make sure things look reasonable"
   ]
  },
  {
   "cell_type": "code",
   "id": "f91bf64e-69a3-41fe-88c1-e452d3b02988",
   "metadata": {},
   "source": [
    "fig = go.Figure(data=go.Volume(\n",
    "    x=x_grid.flatten(), y=y_grid.flatten(), z=z_grid.flatten(),\n",
    "    value=density_list[10].flatten(),\n",
    "    opacity=0.1,\n",
    "    isomin=0.1,\n",
    "    surface_count=25,\n",
    "    ))\n",
    "fig.update_layout(scene_xaxis_showticklabels=False,\n",
    "                  scene_yaxis_showticklabels=False,\n",
    "                  scene_zaxis_showticklabels=False)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed0b11ad-79aa-4601-91c4-419bc1c6eabb",
   "metadata": {},
   "source": [
    "## Obtain Ball harmonics decompositions for each fin density"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d182e96-bd73-4457-ba72-9657d8c4c3ac",
   "metadata": {},
   "source": [
    "bw_harmonic = 25 # Let's try to keep things relatively compact...\n",
    "eps = 1e-6     #desired accuracy\n",
    "N = int(np.ceil(2*max_dim / res)) + 1\n",
    "\n",
    "fle_vec = []\n",
    "coeffs_vec = []\n",
    "vol_vec = []\n",
    "loss_vec = []\n",
    "\n",
    "\n",
    "for n, probs in enumerate(tqdm(density_list)):\n",
    "\n",
    "    prob_grid = np.reshape(probs, (N, N, N))\n",
    "    \n",
    "    fle = FLEBasis3D(N, bw_harmonic, eps, force_real=True)\n",
    "    coeffs = fle.evaluate_t(prob_grid)\n",
    "    volume = fle.evaluate(coeffs)\n",
    "\n",
    "    # record values\n",
    "    fle_vec.append(fle)\n",
    "    coeffs_vec.append(coeffs)\n",
    "    vol_vec.append(volume)\n",
    "\n",
    "    # calculate MSE\n",
    "    loss = np.linalg.norm(probs.flatten() - volume.flatten() )\n",
    "    loss_vec.append(loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16f7d4b6-36c1-4026-accf-5d03aeb769a2",
   "metadata": {},
   "source": [
    "#### Now let's experiment with using UMAP and PCA to capture differences between fins"
   ]
  },
  {
   "cell_type": "code",
   "id": "c138d371-913e-4dfc-8a51-5ac6555431f0",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "coeff_array = np.absolute(np.asarray(coeffs_vec))**2\n",
    "\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(coeff_array)\n",
    "fin_pca_array = pca.transform(coeff_array)\n",
    "fin_pca_array.shape\n",
    "\n",
    "# get cumulative explained variance\n",
    "var_cumu = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "px.scatter(x=range(n_components), y=var_cumu)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4398644-f8c4-435e-b96e-1a3ff4e6af53",
   "metadata": {},
   "source": [
    "# Plot first two PCAs. Color according to x warp strength\n",
    "\n",
    "fig = px.scatter_3d(x=fin_pca_array[:, 0], y=fin_pca_array[:, 1], z=fin_pca_array[:, 2], color=warp_df.loc[:, \"z_warp\"])\n",
    "fig.add_trace(go.Scatter3d(x=[fin_pca_array[0, 0]], y=[fin_pca_array[0, 1]], z=[fin_pca_array[0, 2]]))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a44b20ae-ce83-4d7d-b59b-83aa24e61acc",
   "metadata": {},
   "source": [
    "fig = px.scatter(x=fin_pca_array[:, 0], y=fin_pca_array[:, 1], color=warp_df.loc[:, \"z_warp\"])\n",
    "fig.add_trace(go.Scatter3d(x=[fin_pca_array[0, 0]], y=[fin_pca_array[0, 1]], mode=\"markers\"))\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d89b5255-6477-4542-8e5f-2f3fc0b72184",
   "metadata": {},
   "source": [
    "warp_df.loc[fin_pca_array[:, 0]<5, :]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c504c81e-5851-49ce-ac31-eb3962919f87",
   "metadata": {},
   "source": [
    "fig = go.Figure(data=go.Volume(\n",
    "    x=x_grid.flatten(), y=y_grid.flatten(), z=z_grid.flatten(),\n",
    "    value=density_list[1].flatten(),\n",
    "    opacity=0.1,\n",
    "    isomin=0.01,\n",
    "    surface_count=25,\n",
    "    ))\n",
    "fig.update_layout(scene_xaxis_showticklabels=False,\n",
    "                  scene_yaxis_showticklabels=False,\n",
    "                  scene_zaxis_showticklabels=False)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dcc452d-d8c6-481f-be89-762609983de8",
   "metadata": {},
   "source": [
    "The second PCA captures z-axis extent. This \"makes sense\", because this is the largest spatial axis"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8646ee4-a714-4a1a-ae64-b64ddda46c2f",
   "metadata": {},
   "source": [
    "fig = px.scatter(x=fin_pca_array[:, 0], y=fin_pca_array[:, 1], color=np.prod(warp_df.loc[:, :], axis=1))\n",
    "fig.add_trace(go.Scatter3d(x=[fin_pca_array[0, 0]], y=[fin_pca_array[0, 1]], mode=\"markers\"))\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29cacceb-b52b-40dc-8017-3b4c7495e3cc",
   "metadata": {},
   "source": [
    "First PCA is predominantly fin volume, which also makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f51663-ae36-457a-a0ee-bcc0d3c3ddc7",
   "metadata": {},
   "source": [
    "#### Look at UMAP"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c6f256b-6dfc-45b2-9576-a266069850c9",
   "metadata": {},
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "reducer = umap.UMAP(n_components=3)\n",
    "embedding = reducer.fit_transform(coeff_array)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c39d18c-615a-46db-abf0-626c2a95512f",
   "metadata": {},
   "source": [
    "fig = px.scatter_3d(x=embedding[:, 0], y=embedding[:, 1], z=embedding[:, 2], color=warp_df[\"y_warp\"])\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1995554-cff9-43e9-8e3c-87f93676dee7",
   "metadata": {},
   "source": [
    "### Add noise to see how it interacts with SH"
   ]
  },
  {
   "cell_type": "code",
   "id": "525b2c51-754d-4f4e-838a-ca18f69ff5eb",
   "metadata": {},
   "source": [
    "n_out_modes = 15\n",
    "n_points = fin_df.shape[0]\n",
    "n_outlier_vec = np.floor(np.logspace(0, np.log10(n_points), n_out_modes)).astype(int)\n",
    "noise_scale_vec = np.linspace(0.1, 10, n_out_modes)\n",
    "\n",
    "noise_list = []\n",
    "noisy_points_list = []\n",
    "np.random.seed(61)\n",
    "\n",
    "\n",
    "for n, n_out in enumerate(tqdm(n_outlier_vec)):\n",
    "    for m, mag_out in enumerate(noise_scale_vec):\n",
    "        out_indices = np.random.choice(range(n_points), n_out, replace=False)\n",
    "        noise_array = np.random.normal(loc=0, scale=mag_out, size=(n_out, 3))\n",
    "        points = raw_points.copy()\n",
    "        points[out_indices, :] += noise_array\n",
    "        noisy_points_list.append(points)\n",
    "        noise_list.append([n_out, mag_out])\n",
    "\n",
    "\n",
    "noise_df = pd.DataFrame(np.squeeze(np.asarray(noise_list)), columns=[\"n_outliers\", \"sigma\"])\n",
    "\n",
    "noise_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d9d098e-430a-4a2e-98a6-ab21f46960e5",
   "metadata": {},
   "source": [
    "### Calculate densities and harmonics for noised point clouds"
   ]
  },
  {
   "cell_type": "code",
   "id": "7a0f33af-576d-419e-9e80-89fdb2ad2dc6",
   "metadata": {},
   "source": [
    "density_list_noise = []\n",
    "\n",
    "for i in tqdm(range(noise_df.shape[0])):\n",
    "    points =noisy_points_list[i]\n",
    "    kde = KernelDensity(bandwidth=kde_bw, kernel=\"gaussian\").fit(points) \n",
    "\n",
    "    probs = np.exp(kde.score_samples(xyz_array))\n",
    "    # counts, _ = np.histogramdd(points, (x_bins, y_bins, z_bins))\n",
    "    probs = probs / np.max(probs)\n",
    "\n",
    "    density_list_noise.append(probs)\n",
    "    \n",
    "    # points = warped_points_list[i]\n",
    "    # counts, _ = np.histogramdd(points, (x_bins, y_bins, z_bins))\n",
    "    # probs = counts / np.max(counts)\n",
    "\n",
    "    # density_list_noise.append(probs)\n",
    "\n",
    "fle_vec_noise = []\n",
    "coeffs_vec_noise = []\n",
    "vol_vec_noise = []\n",
    "loss_vec_noise = []\n",
    "\n",
    "\n",
    "for n, probs in enumerate(tqdm(density_list_noise)):\n",
    "\n",
    "    prob_grid = np.reshape(probs, (N, N, N))\n",
    "    \n",
    "    fle = FLEBasis3D(N, bw_harmonic, eps, force_real=True)\n",
    "    coeffs = fle.evaluate_t(prob_grid)\n",
    "    volume = fle.evaluate(coeffs)\n",
    "\n",
    "    # record values\n",
    "    fle_vec_noise.append(fle)\n",
    "    coeffs_vec_noise.append(coeffs)\n",
    "    vol_vec_noise.append(volume)\n",
    "\n",
    "    # calculate MSE\n",
    "    loss = np.linalg.norm(probs.flatten() - volume.flatten() )\n",
    "    loss_vec_noise.append(loss)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4abec046-82ca-4d5d-be7e-5495d080a84c",
   "metadata": {},
   "source": [
    "Spot-check...how different do noised versions look?"
   ]
  },
  {
   "cell_type": "code",
   "id": "18c2463d-9326-4f3f-9add-9d3deb19b715",
   "metadata": {},
   "source": [
    "coeff_array = np.absolute(np.asarray(coeffs_vec + coeffs_vec_noise))**2\n",
    "\n",
    "n_components = 50\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(coeff_array)\n",
    "fin_pca_array = pca.transform(coeff_array)\n",
    "fin_pca_array.shape\n",
    "\n",
    "# get cumulative explained variance\n",
    "var_cumu = np.cumsum(pca.explained_variance_ratio_) * 100\n",
    "\n",
    "px.scatter(x=range(n_components), y=var_cumu)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c437b3f-9a43-44f6-a33d-d6a452bd152d",
   "metadata": {},
   "source": [
    "fig = px.scatter(x=fin_pca_array[100:, 0], y=fin_pca_array[100:, 1], color=noise_df[\"n_outliers\"], size=noise_df[\"sigma\"])\n",
    "fig.add_trace(go.Scatter(x=fin_pca_array[:100, 0], y=fin_pca_array[:100, 1], mode=\"markers\", opacity=0.2))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11618746-d4d0-44e9-9fb1-099390caa1dd",
   "metadata": {},
   "source": [
    "fig = px.scatter(x=fin_pca_array[100:, 0], y=fin_pca_array[100:, 1], color=noise_df[\"n_outliers\"], opacity=0.1)\n",
    "fig.add_trace(go.Scatter(x=[fin_pca_array[0, 0]], y=[fin_pca_array[0, 1]], mode=\"markers\", opacity=0.6))\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55c74dad-80e8-4ed4-8f33-e65fd79fe6f8",
   "metadata": {},
   "source": [
    "### This looks promising! More work needs to be dne, but this indicates a significant level of noise-robustness "
   ]
  },
  {
   "cell_type": "code",
   "id": "b9160f0c-bb90-40e8-8089-1dd6b718e00a",
   "metadata": {},
   "source": [
    "bw = 25\n",
    "l_filter = fle.ls <= bw\n",
    "total_power = np.sqrt(np.sum(coeff_array[0, l_filter]))\n",
    "\n",
    "shape_dist_vec = np.sqrt(np.sum((np.sqrt(coeff_array[:, l_filter])-np.sqrt(coeff_array[0, l_filter]))**2, axis=1)) / total_power\n",
    "fig = px.scatter(noise_df, x=\"sigma\", y=\"n_outliers\", color=shape_dist_vec[100:], opacity=1)\n",
    "fig.update_traces(marker=dict(size=16))\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_type=\"log\")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7dce42b4-c46d-47f9-9a08-9ca9dc951bdf",
   "metadata": {},
   "source": [
    "shape_dist_vec = np.sqrt(np.sum((np.sqrt(coeff_array[:, l_filter])-np.sqrt(coeff_array[0, l_filter]))**2, axis=1)) / total_power\n",
    "warp_df[\"x_warp_log\"] = np.log10(warp_df[\"x_warp\"])\n",
    "warp_df[\"y_warp_log\"] = np.log10(warp_df[\"y_warp\"])\n",
    "warp_df[\"z_warp_log\"] = np.log10(warp_df[\"z_warp\"])\n",
    "\n",
    "fig = px.scatter_3d(warp_df, x=\"x_warp_log\", y=\"y_warp_log\", z=\"z_warp_log\", color=shape_dist_vec[:100], opacity=1)\n",
    "fig.update_traces(marker=dict(size=6))\n",
    "\n",
    "# fig.update_layout(\n",
    "#     yaxis_type=\"log\",\n",
    "#     xaxis_type=\"log\")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a21a9d0-326f-42fc-a16d-25418654fe64",
   "metadata": {},
   "source": [
    "ind = np.where((np.round(noise_df[\"sigma\"],1)==8.6) & (noise_df[\"n_outliers\"]==3))[0][0]\n",
    "print(ind)\n",
    "fig = px.scatter_3d(x=noisy_points_list[ind][:, 0], y=noisy_points_list[ind][:, 1], z=noisy_points_list[ind][:, 2])\n",
    "fig.add_trace(go.Scatter3d(x=warped_points_list[0][:, 0], y=warped_points_list[0][:, 1], z=warped_points_list[0][:, 2], mode=\"markers\"))\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c23087d-e04f-4237-8ae0-0876d737afb2",
   "metadata": {},
   "source": [
    "noisy_points_list[0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0b3db79c-982b-4518-b618-010817c597b5",
   "metadata": {},
   "source": [
    "#### What does the average across all warps look like?"
   ]
  },
  {
   "cell_type": "code",
   "id": "d33cde52-44ff-4141-8480-371af905ab21",
   "metadata": {},
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fle = FLEBasis3D(N, bw_harmonic, eps, force_real=True)\n",
    "\n",
    "coeff_array_full = np.asarray(coeffs_vec)\n",
    "coeffs_mean = np.median(coeff_array_full, axis=0)\n",
    "vol_mean = fle.evaluate(coeffs_mean)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "fig.add_trace(go.Contour(z=vol_mean[:, 37, :], contours=dict(start=0.05, end=0.75), showscale=False), 1, 1)\n",
    "fig.add_trace(go.Contour(z=vol_vec[0][:, 37, :], contours=dict(start=0.05, end=0.75), showscale=False), 1, 2)\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad310cb9-12b8-4aa1-b73d-868f256739c1",
   "metadata": {},
   "source": [
    "fig = make_subplots(rows=1, cols=2, specs=[[{'type': 'volume'}, {'type': 'volume'}]])\n",
    "\n",
    "\n",
    "fig.add_trace(go.Volume(\n",
    "    x=x_grid.flatten(), y=y_grid.flatten(), z=z_grid.flatten(),\n",
    "    value=vol_mean.flatten(),\n",
    "    opacity=0.1,\n",
    "    isomin=0.05,\n",
    "    isomax=0.7,\n",
    "    surface_count=25,\n",
    "    ), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Volume(\n",
    "    x=x_grid.flatten(), y=y_grid.flatten(), z=z_grid.flatten(),\n",
    "    value=vol_vec[0].flatten(),\n",
    "    opacity=0.1,\n",
    "    isomin=0.05,\n",
    "    isomax=0.7,\n",
    "    surface_count=25,\n",
    "    ), row=1, col=2)\n",
    "\n",
    "fig.update_layout(scene_xaxis_showticklabels=False,\n",
    "                  scene_yaxis_showticklabels=False,\n",
    "                  scene_zaxis_showticklabels=False,\n",
    "                 scene=dict( aspectmode='data'\n",
    "                 ))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eff73387-cc5b-48a3-abbe-cffdea24b93e",
   "metadata": {},
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Isosurface(\n",
    "    x=x_grid.flatten(), y=y_grid.flatten(), z=z_grid.flatten(),\n",
    "    value=vol_mean.flatten(),\n",
    "    opacity=0.5,\n",
    "    isomin=0.05,\n",
    "    isomax=0.7,\n",
    "    surface_count=5,\n",
    "    ))\n",
    "\n",
    "fig.update_layout(scene_xaxis_showticklabels=False,\n",
    "                  scene_yaxis_showticklabels=False,\n",
    "                  scene_zaxis_showticklabels=False,\n",
    "                 scene=dict( aspectmode='data'\n",
    "                 ))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "23274b67-3afe-4a9d-83a0-b937e52df85c",
   "metadata": {},
   "source": [
    "This reconstruction actually looks quite \"good\" for the H-R sampling, even though it has a bw of just 10. Clearly, MSE is not the ideal metric for measuring how \"close\" a reconstruction is. \n",
    "\n",
    "Also, feeding a more information-rich distribution in may be fine, so long as I don't expect to obtain a perfect reconstruction. The harmonics porivde a natural way to filter for larger-scale features. "
   ]
  },
  {
   "cell_type": "code",
   "id": "70d0da2d-0334-41c3-ab88-ef685012aef9",
   "metadata": {},
   "source": [
    "# calculate loading arrays for each fin\n",
    "l_vec = np.asarray(fle01.ls)\n",
    "l_index = np.unique(l_vec)\n",
    "m_vec = np.asarray(fle01.ms)\n",
    "m_index = np.unique(m_vec)\n",
    "k_vec = np.asarray(fle01.ks)\n",
    "k_index = np.unique(k_vec)\n",
    "\n",
    "power_array01 = np.empty((len(l_index), len(k_index)))\n",
    "power_array01_rot = np.empty((len(l_index), len(k_index)))\n",
    "power_array02 = np.empty((len(l_index), len(k_index)))\n",
    "\n",
    "mags01 = np.absolute(coeff01)**2\n",
    "mags01_rot = np.absolute(coeff01_rot)**2\n",
    "mags02 = np.absolute(coeff02)**2\n",
    "for li, l in enumerate(l_index):\n",
    "    for ki, k in enumerate(k_index):\n",
    "        indices = (l_vec==l) & (k_vec==k)\n",
    "        power_array01[li, ki] = np.sum(mags01[indices])\n",
    "        power_array01_rot[li, ki] = np.sum(mags01_rot[indices])\n",
    "        power_array02[li, ki] = np.sum(mags02[indices])\n",
    "\n",
    "# for li, l in enumerate(l_index):\n",
    "#     for mi, m in enumerate(k_index):\n",
    "#         indices = (l_vec==l) & (m_vec==m)\n",
    "#         m_array[li, mi] = np.sum(mags[indices])\n",
    "#         m_array2[li, mi] = np.sum(mags2[indices])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d4e61883-b0a5-43df-a998-a4f23fc03c0f",
   "metadata": {},
   "source": [
    "px.imshow(np.log10(power_array01+1e-10))\n",
    "# np.all(fle.ms == fle2.ms)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "459da9f9-6733-466f-979c-c76a3505b8dd",
   "metadata": {},
   "source": [
    "print(np.sqrt(np.sum((power_array01)**2)))\n",
    "print(np.sqrt(np.sum((power_array01-power_array01_rot)**2)))\n",
    "print(np.sqrt(np.sum((power_array01-power_array02)**2)))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8bb18c2f-b57d-4d92-8df6-49a0c4f4ccb3",
   "metadata": {},
   "source": [
    "# calculate loading arrays for each fin\n",
    "l_vec = np.asarray(fle0.ls)\n",
    "l_index = np.unique(l_vec)\n",
    "k_vec = np.asarray(fle0.ks)\n",
    "k_index = np.unique(k_vec)\n",
    "power_array_list = []\n",
    "power_vec_list = []\n",
    "for i in tqdm(range(len(sph_coeff_list))):\n",
    "    power_array = np.empty((len(l_index), len(k_index)))\n",
    "    coeffs = sph_coeff_list[i]\n",
    "    mags = np.absolute(coeffs)**2\n",
    "    for li, l in enumerate(l_index):\n",
    "        for ki, k in enumerate(k_index):\n",
    "            indices = (l_vec==l) & (k_vec==k)\n",
    "            power_array[li, ki] = np.sum(mags[indices])\n",
    "\n",
    "    power_array_list.append(power_array)\n",
    "    power_vec_list.append(power_array.ravel())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5e53e8b-434b-4ebf-b655-2695ae48ad10",
   "metadata": {},
   "source": [
    "power_array.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d724969-a209-4d7c-82f0-00d8051da7a5",
   "metadata": {},
   "source": [
    "recon_bw = 25\n",
    "\n",
    "file_ind = 1\n",
    "fp = fin_object_list[file_ind]\n",
    "point_prefix = path_leaf(fp).replace(\"_fin_object.pkl\", \"\")\n",
    "print(point_prefix)\n",
    "\n",
    "fin_data = FinData(data_root=root, name=point_prefix, tissue_seg_model=seg_type)\n",
    "\n",
    "# get fin dataset\n",
    "fin_df = fin_data.full_point_data\n",
    "fin_df = fin_df.loc[fin_df[\"fin_label_curr\"]==1, :]\n",
    "\n",
    "# fit density function # fit density\n",
    "X = fin_df[[\"X\", \"Y\", \"Z\"]].to_numpy()\n",
    "X = X - np.mean(X, axis=0)\n",
    "kde = KernelDensity(bandwidth=kde_bandwidth, kernel=\"gaussian\").fit(X)\n",
    "\n",
    "# get density grid\n",
    "score_array = kde.score_samples(test_array)\n",
    "score_array_thresh = score_array.copy()\n",
    "prob_array[prob_array < -5] = -np.inf\n",
    "prob_array = np.exp(score_array_thresh)\n",
    "prob_grid = np.reshape(prob_array, x_grid.shape)\n",
    "prob_grid = prob_grid / np.max(np.abs(prob_grid))\n",
    "\n",
    "# fit transform\n",
    "N = prob_grid.shape[0]  #replace this by the side-length of your volume array\n",
    "# bandlimit = 25   #maximum number of basis functions to use\n",
    "\n",
    "fle0 = FLEBasis3D(N, recon_bw, eps, force_real=True)\n",
    "coeff = fle0.evaluate_t(prob_grid)\n",
    "volume = fle0.evaluate(coeff)\n",
    "weigths = fle0.weights\n",
    "\n",
    "\n",
    "print(weight)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18bf40ba-f610-426c-8f72-873c3346c756",
   "metadata": {},
   "source": [
    "bandlimit = 25   #maximum number of basis functions to use\n",
    "eps = 1e-7      #desired accuracy\n",
    "fle = FLEBasis3D(N, bandlimit, eps, force_real=True)\n",
    "      FLEBasis3D(N, recon_bw, eps, force_real=True)\n",
    "# fle.force_real=True\n",
    "coeff = fle.evaluate_t(prob_grid)\n",
    "volume = fle.evaluate(coeff)\n",
    "weights = fle.weights\n",
    "\n",
    "print(weights)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46f675ea-d7c6-43fa-9633-f78349bab997",
   "metadata": {},
   "source": [
    "test = fle.create_denseB()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5c743c9-39fa-4c93-9c25-71a0463c0f0c",
   "metadata": {},
   "source": [
    "test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dde1d1b9-3f1c-4a70-82fc-0fc5b5f2c2ca",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-shape-env)",
   "language": "python",
   "name": "fin-shape-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
