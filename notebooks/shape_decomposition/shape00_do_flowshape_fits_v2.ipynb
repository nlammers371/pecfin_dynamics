{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c8bfcd-f710-4c3e-a6ac-f5a0eea113c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowshape as fs\n",
    "import igl\n",
    "import numpy as np\n",
    "import meshplot as mp\n",
    "import os\n",
    "import pandas as pd\n",
    "from src.utilities.fin_shape_utils import plot_mesh\n",
    "from src.utilities.fin_class_def import FinData\n",
    "from src.utilities.functions import path_leaf\n",
    "import glob2 as glob\n",
    "import meshplot as mp\n",
    "import trimesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8241cfc-05d1-488b-b346-a2a436f6a2fe",
   "metadata": {},
   "source": [
    "## First pass did not yield great results\n",
    "One possible reason is that the SH decomposition was not encoding any size info. I want to see if it is possible to do this by altering the scalar cuvature field by a size factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a673ae-5a8b-49e6-943d-972616c01c49",
   "metadata": {},
   "source": [
    "### Load fin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe2191ac-77f5-448f-969e-9850fde8372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of refined fin mesh objects\n",
    "root = \"/media/nick/hdd02/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/pecfin_dynamics/\"\n",
    "fin_mesh_list = sorted(glob.glob(os.path.join(root, \"point_cloud_data\", \"processed_fin_data\", \"*smoothed_fin_mesh*\")))\n",
    "\n",
    "# load metadata\n",
    "metadata_df = pd.read_csv(os.path.join(root, \"metadata\", \"master_metadata.csv\"))\n",
    "metadata_df[\"experiment_date\"] = metadata_df[\"experiment_date\"].astype(str)\n",
    "metadata_df.head()\n",
    "\n",
    "# make savedir \n",
    "save_dir = os.path.join(root, \"point_cloud_data\", \"SH_analysis_v2\", \"\")\n",
    "y_subdir = os.path.join(save_dir, \"Y_matrices\")\n",
    "if not os.path.isdir(y_subdir):\n",
    "    os.makedirs(y_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfc7da-b4dd-4d81-84a2-47c1925658d3",
   "metadata": {},
   "source": [
    "### This is the norm utility in flowshape, but returns info I need for recovering original fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91149cd4-7bd3-466a-b7d3-d9c3d9dee5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def norm_verbose(verts):\n",
    "    centroid = np.mean(verts, axis=0)\n",
    "    verts -= centroid\n",
    "    radii = norm(verts, axis=1)\n",
    "\n",
    "    m = np.amax(radii)\n",
    "\n",
    "    verts /= m\n",
    "    return verts, centroid, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0386a36b-d471-4fc4-a1ad-c68b9ee65810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make col names to keep track of SH coefficients\n",
    "\n",
    "def get_sh_colnames(max_degree):\n",
    "\n",
    "    col_names = []\n",
    "    \n",
    "    for l in range(max_degree):\n",
    "        \n",
    "        i1 = l**2\n",
    "        i2 = (l + 1) ** 2\n",
    "    \n",
    "        for m in range(i1, i2):\n",
    "            col_name = f\"sh_l{l:03}_m{m-i1:03}\"\n",
    "            col_names.append(col_name)\n",
    "            \n",
    "    return col_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6d211-1401-42e6-baf9-8b36538b9eb4",
   "metadata": {},
   "source": [
    "### Iterate through fin meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99db1539-ca52-4242-8202-6c602b8d3e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/224 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m v_bary \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mproject_sphere(v_bary)\n\u001b[1;32m     58\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m igl\u001b[38;5;241m.\u001b[39mdoublearea(v_sphere, f)\n\u001b[0;32m---> 59\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mdiags(W)\n\u001b[1;32m     60\u001b[0m weights, Y_mat \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mIRF_scalar(rho_scaled, v_bary, W, max_degree\u001b[38;5;241m=\u001b[39ml_max)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# perform SH decomposition\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# weights, Y_mat, vs = fs.do_mapping(vn, f, l_max=max_degree)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# store results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sp' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_degree = 30\n",
    "\n",
    "df_list = []\n",
    "for file_ind, file_path in enumerate(tqdm(fin_mesh_list)):\n",
    "    # get colnames\n",
    "    sh_colnames = get_sh_colnames(max_degree)\n",
    "    \n",
    "    # extract relevant metadata\n",
    "    fname = os.path.basename(file_path)\n",
    "    well_ind = fname.find(\"well\")\n",
    "    date_string = fname[:well_ind-1]\n",
    "    well_num = int(fname[well_ind+4:well_ind+8])\n",
    "    time_ind = fname.find(\"time\")\n",
    "    time_num = int(fname[time_ind+4:time_ind+8])\n",
    "    \n",
    "    # match this to a row in the metadata df\n",
    "    date_ft = metadata_df[\"experiment_date\"] == date_string\n",
    "    well_ft = metadata_df[\"well_index\"] == well_num\n",
    "    time_ft = metadata_df[\"time_index\"] == time_num\n",
    "    \n",
    "    meta_temp = metadata_df.loc[date_ft & well_ft & time_ft, :].reset_index(drop=True)\n",
    "\n",
    "    cv = meta_temp.loc[0, \"chem_i\"]\n",
    "    if isinstance(cv, str):\n",
    "        cvs = cv.split(\"_\")\n",
    "        chem_id = cvs[0]\n",
    "        chem_time = int(cvs[1])\n",
    "    else:\n",
    "        chem_id = \"WT\"\n",
    "        chem_time = np.nan\n",
    "    meta_temp.loc[0, \"chem_id\"] = chem_id\n",
    "    meta_temp.loc[0, \"chem_time\"] = chem_time\n",
    "    \n",
    "    # make temp DF to store results\n",
    "\n",
    "    # try:\n",
    "    # load mesh\n",
    "    fin_mesh = trimesh.load(file_path)\n",
    "    f, v = fin_mesh.faces.copy(), fin_mesh.vertices.copy()\n",
    "    \n",
    "    # normalize\n",
    "    vn, mu, m = norm_verbose(v.copy())\n",
    "\n",
    "    # map to sphere\n",
    "    v_sphere = fs.sphere_map(vn, f)\n",
    "\n",
    "    # calculate curvature\n",
    "    rho = fs.curvature_function(vn, v_sphere, f)\n",
    "    # rho_norm = fs.curvature_function(vn, v_sphere, f)\n",
    "\n",
    "    # adjust to reflect overall scale of the shape\n",
    "    rho_scaled = rho / m\n",
    "    \n",
    "    v_bary = igl.barycenter(v_sphere, f)\n",
    "    v_bary = fs.project_sphere(v_bary)\n",
    "    W = 0.5 * igl.doublearea(v_sphere, f)\n",
    "    W = sp.sparse.diags(W)\n",
    "    weights, Y_mat = fs.IRF_scalar(rho_scaled, v_bary, W, max_degree=l_max)\n",
    "\n",
    "    # perform SH decomposition\n",
    "    # weights, Y_mat, vs = fs.do_mapping(vn, f, l_max=max_degree)\n",
    "    \n",
    "    # store results\n",
    "    meta_temp.loc[0, \"scale\"] = m.copy()\n",
    "    meta_temp.loc[0, [\"xc\", \"yc\", \"zc\"]] = mu.copy()\n",
    "    meta_temp.loc[0, sh_colnames] = weights.copy()\n",
    "\n",
    "    df_list.append(meta_temp)\n",
    "\n",
    "    # save Y\n",
    "    yname = fname\n",
    "    yname.replace(\"_smoothed_fin_mesh.obj\", \"y_mat.npy\")\n",
    "    np.save(os.path.join(y_subdir, yname), Y_mat)\n",
    "        \n",
    "    # # except:\n",
    "    # #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13bff0ad-ad7f-40a5-8b22-a4b70e0f966d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# SH_df = pd.concat(df_list, axis=0, ignore_index=True)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# SH_df.to_csv(os.path.join(save_dir, \"fin_sh_df.csv\"), index=False)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mY_mat\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_mat' is not defined"
     ]
    }
   ],
   "source": [
    "# SH_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "# SH_df.to_csv(os.path.join(save_dir, \"fin_sh_df.csv\"), index=False)\n",
    "Y_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522383b-e9a4-4037-a5b9-68b40ab07b0d",
   "metadata": {},
   "source": [
    "### Align shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346646fe-cc20-409b-90c7-7d44acec6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract array of SH coefficients\n",
    "max_degree = 30\n",
    "sh_cols, _, _ = get_sh_colnames(max_degree=max_degree)\n",
    "sh_array = SH_df.loc[:, sh_cols].to_numpy()\n",
    "\n",
    "# extract reference\n",
    "well_ref = 46\n",
    "time_ref = 0\n",
    "date_ref = \"20240711_01\"\n",
    "\n",
    "# get location of the reference\n",
    "date_ft = SH_df[\"experiment_date\"] == date_ref\n",
    "well_ft = SH_df[\"well_index\"] == well_ref\n",
    "time_ft = SH_df[\"time_index\"] == time_ref\n",
    "\n",
    "ref_row = SH_df.loc[date_ft & well_ft & time_ft, :].reset_index(drop=True)\n",
    "ref_sh_weights = ref_row.loc[0, sh_cols].to_numpy().astype('complex128')\n",
    "\n",
    "sh_array_aligned = np.zeros_like(sh_array)\n",
    "# iterate through and align\n",
    "for i in tqdm(range(sh_array.shape[0])):\n",
    "    # get ceoffs\n",
    "    sh2 = sh_array[i, :].astype('complex128')\n",
    "    # compute alignment\n",
    "    rot2 = fs.compute_max_correlation(ref_sh_weights, sh2, l_max=max_degree)\n",
    "    # apply alignment to weights\n",
    "    sh_array_aligned[i, :] = fs.rotate_weights(rot2, sh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f406b0-e715-4ed2-8c0e-dc9045446d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SH_df_a = SH_df.copy()\n",
    "SH_df_a.loc[:, sh_cols] = sh_array_aligned\n",
    "SH_df_a.to_csv(os.path.join(sh_dir, \"fin_sh_df_aligned.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685e4bb-ddeb-4950-be3f-a700daaebe71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flowshape-env)",
   "language": "python",
   "name": "flowshape-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
