{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Point Net - Classification**\n",
    "\n",
    "In this notebook, we will use the classification version of point net to classify objects from the shapenet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "import open3d as o3\n",
    "\n",
    "from open3d.web_visualizer import draw # for non Colab\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# TEMP for supressing pytorch user warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Colab you can run this cell to download the dataset. If you would like to download the dataset to your PC visit the url and the dataset will automatically download as a zip \n",
    "\n",
    "WARNING: Downloading the dataset will take a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Run this only if you don't already have the Dataset\n",
    "# !wget -nv https://shapenet.cs.stanford.edu/ericyi/shapenetcore_partanno_segmentation_benchmark_v0.zip --no-check-certificate\n",
    "# !unzip shapenetcore_partanno_segmentation_benchmark_v0.zip\n",
    "# !rm shapenetcore_partanno_segmentation_benchmark_v0.zip"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# General parameters\n",
    "NUM_TRAIN_POINTS = 2500\n",
    "NUM_TEST_POINTS = 10000\n",
    "NUM_CLASSES = 16\n",
    "ROOT = r'C:\\Users\\itber\\Documents\\datasets\\shapenetcore_partanno_segmentation_benchmark_v0'\n",
    "\n",
    "# model hyperparameters\n",
    "GLOBAL_FEATS = 1024\n",
    "\n",
    "BATCH_SIZE = 32"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# get class - label mappings\n",
    "CATEGORIES = {\n",
    "    'Airplane': 0, \n",
    "    'Bag': 1, \n",
    "    'Cap': 2, \n",
    "    'Car': 3,\n",
    "    'Chair': 4, \n",
    "    'Earphone': 5, \n",
    "    'Guitar': 6, \n",
    "    'Knife': 7, \n",
    "    'Lamp': 8, \n",
    "    'Laptop': 9,\n",
    "    'Motorbike': 10, \n",
    "    'Mug': 11, \n",
    "    'Pistol': 12, \n",
    "    'Rocket': 13, \n",
    "    'Skateboard': 14, \n",
    "    'Table': 15}\n",
    "            \n",
    "# Simple point cloud coloring mapping for part segmentation\n",
    "def read_pointnet_colors(seg_labels):\n",
    "    map_label_to_rgb = {\n",
    "        1: [0, 255, 0],\n",
    "        2: [0, 0, 255],\n",
    "        3: [255, 0, 0],\n",
    "        4: [255, 0, 255],  # purple\n",
    "        5: [0, 255, 255],  # cyan\n",
    "        6: [255, 255, 0],  # yellow\n",
    "    }\n",
    "    colors = np.array([map_label_to_rgb[label] for label in seg_labels])\n",
    "    return colors"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from shapenet_dataset import ShapenetDataset\n",
    "\n",
    "# train Dataset & DataLoader\n",
    "train_dataset = ShapenetDataset(ROOT, npoints=NUM_TRAIN_POINTS, split='train', classification=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Validation Dataset & DataLoader\n",
    "valid_dataset = ShapenetDataset(ROOT, npoints=NUM_TRAIN_POINTS, split='valid', classification=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# test Dataset & DataLoader \n",
    "test_dataset = ShapenetDataset(ROOT, npoints=NUM_TEST_POINTS, split='test', classification=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# test Dataset  (segmentation version for display)\n",
    "test_sample_dataset = ShapenetDataset(ROOT, npoints=NUM_TEST_POINTS, split='test', \n",
    "                                      classification=False, normalize=False)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "sample_dataset = ShapenetDataset(ROOT, npoints=20000, split='train', \n",
    "                                 classification=False, normalize=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "points, seg = sample_dataset[4000]\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(points)\n",
    "pcd.colors = o3.utility.Vector3dVector(read_pointnet_colors(seg.numpy()))\n",
    "\n",
    "o3.visualization.draw_plotly([pcd])\n",
    "\n",
    "# optional visualization for non Colab\n",
    "# draw(pcd)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore class frequencies and plot barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "total_train_targets = []\n",
    "for (_, targets) in train_dataloader:\n",
    "    total_train_targets += targets.reshape(-1).numpy().tolist()\n",
    "\n",
    "total_train_targets = np.array(total_train_targets)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "class_bins = np.bincount(total_train_targets)\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), class_bins, \n",
    "             color=mpl.cm.tab20(np.arange(0, NUM_CLASSES)),\n",
    "             edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
    "plt.ylabel('Counts', size=12)\n",
    "plt.title('Train Class Frequencies', size=14, pad=20);"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "from point_net import PointNetClassHead\n",
    "\n",
    "points, targets = next(iter(train_dataloader))\n",
    "\n",
    "classifier = PointNetClassHead(k=NUM_CLASSES, num_global_feats=GLOBAL_FEATS)\n",
    "out, _, _ = classifier(points.transpose(2, 1))\n",
    "print(f'Class output shape: {out.shape}')"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss for PointNet\n",
    "\n",
    "For Point Net we will use the Categorical Cross Entropy loss with a regularization term that will enforce the high dimensional transform matrix to 0. We will also present the option for using the Balanced Cross Entropy loss via the 'alpha' argument which assigns a weight to each class this weights the importance of each example based on their class frequencies. We also provide the option to use the Focal loss which adds a modulating term to the Cross Entropy Loss $(1 - p_n)^\\gamma$, this term forces the model to focus on hard examples (i.e. examples with low prediction probability). Some notes on the Focal Loss are given below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focal Loss Notes\n",
    "The Focal Loss is a modified Cross Entropy (CE) Loss. The CE Loss for a sample $n$ is given below. \n",
    "\n",
    "$$ CE(s_n, y_n) = -\\alpha_{y_n} log \\left( \\frac{exp(\\mathbf{s_n})}{\\sum_{c=1}^M exp(s_{n,c})} \\right) \\mathbf{1}_C(y_n) $$\n",
    "\n",
    "Where $\\mathbf{s_n}$ is the predicted class score vector (logits), $M$ is the number of classes, $y_n$ is the true class, $\\alpha_{y_n}$ is the class weight, and $\\mathbf{1}_C(y_n)$ is the [Indcator Function](https://en.wikipedia.org/wiki/Indicator_function) that tells us to only consider the prediction for the current class $y_n$. <br>\n",
    "$e.g.$ if $\\mathbf{s_n} = [0.02, 0.142, 0.356, 0.0012, 0.0456]$ and $y_n = 3$, then $\\mathbf{s_n}  \\mathbf{1}_C(y_n) = 0.0012$ (NOTE: we used zero indexing here).\n",
    "\n",
    "We may also notice that the term inside the $log$ is the Softmax function, which along wth the indicator function, gives us the predicted class probability. We can rewrite the CE Loss in a more simple format as:\n",
    "$$ CE(p_n) = -\\alpha_{y_n} log(p_n)  $$\n",
    "Where the $n$ subscript refers to the true class at sample $n$.\n",
    "\n",
    "<br> \n",
    "The CE loss is typically unweighted, but if a weight is used it is reffered to as the Balanced (or Weighted) CE Loss. The weights are usually based on inverse class distribution and typically range from [0, 1]. The weights can also be set as a hyperparameter via Cross Validation.\n",
    "\n",
    "$$ \\mathbf{\\alpha} = \\frac{1}{\\text{class counts}} $$\n",
    "We may also normalize alpha so that it spreads between [0,1]\n",
    "$$ \\alpha = \\frac{\\alpha}{\\max{\\alpha}} $$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The Focal Loss adds an additional modulating factor to the weighted CE Loss: $ (1 - p_n)^\\gamma $, where $\\gamma \\geq 0$ is referred to as the focusing parameter. This term has tends to 0 when the prediction probability is high, and has a larger value when the prediction probability is lower forcing the model to focus on the hard examples. It can be said that the focusing parameter smoothly adjusts the rate at which easy examples are down-weighted. We can formally express the Focal Loss as:\n",
    "\n",
    "$$ FL(p_n) = -\\alpha_{y_n} (1 - p_n)^\\gamma log(p_n)  $$\n",
    "\n",
    "<br> Summary: <br>\n",
    "- The Focal Loss is a modified version of the Balanced Cross Entropy Loss\n",
    "- The class weight $\\alpha$ balances the importance of loss values based class distribution\n",
    "- The modulating term forces the model to focus on the hard examples by downweighting the easy examples\n",
    "increasing the loss of predictions with lower probability.\n",
    "- The class weights $\\alpha$ and focusing parameter $\\gamma$ are hyperparameters that can be tuned \n",
    "\n",
    "<br>\n",
    "Sources: <br>\n",
    "\n",
    "- https://gombru.github.io/2018/05/23/cross_entropy_loss/ \n",
    "- https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "- https://hasty.ai/docs/mp-wiki/loss/focal-loss\n",
    "- https://arxiv.org/pdf/1708.02002.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "from point_net_loss import PointNetLoss\n",
    "\n",
    "EPOCHS = 100\n",
    "LR = 0.0001\n",
    "REG_WEIGHT = 0.001 \n",
    "\n",
    "# use inverse class weighting\n",
    "# alpha = 1 / class_bins\n",
    "# alpha = (alpha/alpha.max())\n",
    "\n",
    "# manually downweight the high frequency classes\n",
    "alpha = np.ones(NUM_CLASSES)\n",
    "alpha[0] = 0.5  # airplane\n",
    "alpha[4] = 0.5  # chair\n",
    "alpha[-1] = 0.5 # table\n",
    "\n",
    "gamma = 2\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01, \n",
    "                                              step_size_up=2000, cycle_momentum=False)\n",
    "criterion = PointNetLoss(alpha=alpha, gamma=gamma, reg_weight=REG_WEIGHT).to(DEVICE)\n",
    "\n",
    "classifier = classifier.to(DEVICE)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be sure we are not misclassifying or only focusing on easy or abundant examples. So we use the Matthews Correlation Coefficient to measure our models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "mcc_metric = MulticlassMatthewsCorrCoef(num_classes=NUM_CLASSES).to(DEVICE)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training Model\n",
    "\n",
    "First define a helper function to train, validate, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "def train_test(classifier, dataloader, num_batch, epoch, split='train'):\n",
    "    ''' Function to train or test the model '''\n",
    "    _loss = []\n",
    "    _accuracy = []\n",
    "    _mcc = []\n",
    "\n",
    "    # return total targets and predictions for test case only\n",
    "    total_test_targets = []\n",
    "    total_test_preds = [] \n",
    "    for i, (points, targets) in enumerate(dataloader, 0):\n",
    "\n",
    "        points = points.transpose(2, 1).to(DEVICE)\n",
    "        targets = targets.squeeze().to(DEVICE)\n",
    "        \n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get predicted class logits\n",
    "        preds, _, A = classifier(points)\n",
    "\n",
    "        # get loss and perform backprop\n",
    "        loss = criterion(preds, targets, A) \n",
    "\n",
    "        if split == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        # get class predictions\n",
    "        pred_choice = torch.softmax(preds, dim=1).argmax(dim=1) \n",
    "        correct = pred_choice.eq(targets.data).cpu().sum()\n",
    "        accuracy = correct.item()/float(BATCH_SIZE)\n",
    "        mcc = mcc_metric(preds, targets)\n",
    "\n",
    "        # update epoch loss and accuracy\n",
    "        _loss.append(loss.item())\n",
    "        _accuracy.append(accuracy)\n",
    "        _mcc.append(mcc.item())\n",
    "\n",
    "        # add to total targets/preds\n",
    "        if split == 'test':\n",
    "            total_test_targets += targets.reshape(-1).cpu().numpy().tolist()\n",
    "            total_test_preds += pred_choice.reshape(-1).cpu().numpy().tolist()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'\\t [{epoch}: {i}/{num_batch}] ' \\\n",
    "                  + f'{split} loss: {loss.item():.4f} ' \\\n",
    "                  + f'accuracy: {accuracy:.4f} mcc: {mcc:.4f}')\n",
    "        \n",
    "    epoch_loss = np.mean(_loss)\n",
    "    epoch_accuracy = np.mean(_accuracy)\n",
    "    epoch_mcc = np.mean(_mcc)\n",
    "\n",
    "    print(f'Epoch: {epoch} - {split} Loss: {epoch_loss:.4f} ' \\\n",
    "          + f'- {split} Accuracy: {epoch_accuracy:.4f} ' \\\n",
    "          + f'- {split} MCC: {epoch_mcc:.4f}')\n",
    "\n",
    "    if split == 'test':\n",
    "        return epoch_loss, epoch_accuracy, epoch_mcc, total_test_targets, total_test_preds\n",
    "    else: \n",
    "        return epoch_loss, epoch_accuracy, epoch_mcc"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# stuff for training\n",
    "num_train_batch = int(np.ceil(len(train_dataset)/BATCH_SIZE))\n",
    "num_valid_batch = int(np.ceil(len(valid_dataset)/BATCH_SIZE))\n",
    "\n",
    "# store best validation mcc above 0.\n",
    "best_mcc = 0.\n",
    "\n",
    "# lists to store metrics (loss, accuracy, mcc)\n",
    "train_metrics = []\n",
    "valid_metrics = []\n",
    "\n",
    "# TRAIN ON EPOCHS\n",
    "for epoch in range(1, EPOCHS):\n",
    "\n",
    "    ## train loop\n",
    "    classifier = classifier.train()\n",
    "    \n",
    "    # train\n",
    "    _train_metrics = train_test(classifier, train_dataloader, \n",
    "                                num_train_batch, epoch, \n",
    "                                split='train')\n",
    "    train_metrics.append(_train_metrics)\n",
    "        \n",
    "\n",
    "    # pause to cool down\n",
    "    time.sleep(4)\n",
    "\n",
    "    ## validation loop\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # place model in evaluation mode\n",
    "        classifier = classifier.eval()\n",
    "\n",
    "        # validate\n",
    "        _valid_metrics = train_test(classifier, valid_dataloader, \n",
    "                                    num_valid_batch, epoch, \n",
    "                                    split='valid')\n",
    "        valid_metrics.append(_valid_metrics)\n",
    "\n",
    "        # pause to cool down\n",
    "        time.sleep(4)\n",
    "\n",
    "    # save model if necessary\n",
    "    if valid_metrics[-1][-1] >= best_mcc:\n",
    "        best_mcc = valid_metrics[-1][-1]\n",
    "        torch.save(classifier.state_dict(), 'trained_models/cls_focal_clr_2/cls_model_%d.pth' % epoch)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "metric_names = ['loss', 'accuracy', 'mcc']\n",
    "_, ax = plt.subplots(len(metric_names), 1, figsize=(8, 6))\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    ax[i].set_title(m)\n",
    "    ax[i].plot(train_metrics[:, i], label='train')\n",
    "    ax[i].plot(valid_metrics[:, i], label='valid')\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.subplots_adjust(wspace=0., hspace=0.35)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "MODEL_PATH = 'trained_models/cls_focal_clr/cls_model_35.pth'\n",
    "\n",
    "classifier = PointNetClassHead(num_points=NUM_TEST_POINTS, num_global_feats=GLOBAL_FEATS, k=NUM_CLASSES).to(DEVICE)\n",
    "classifier.load_state_dict(torch.load(MODEL_PATH))\n",
    "classifier.eval();"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run test loop and get confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "num_test_batch = int(np.ceil(len(test_dataset)/BATCH_SIZE))\n",
    "\n",
    "with torch.no_grad():\n",
    "    epoch_loss, \\\n",
    "    epoch_accuracy, \\\n",
    "    epoch_mcc, \\\n",
    "    total_test_targets, \\\n",
    "    total_test_preds = train_test(classifier, test_dataloader, \n",
    "                              num_test_batch, epoch=1, \n",
    "                              split='test')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "print(f'Test Loss: {epoch_loss:.4f} ' \\\n",
    "      f'- Test Accuracy: {epoch_accuracy:.4f} ' \\\n",
    "      f'- Test MCC: {epoch_mcc:.4f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "test_confusion = pd.DataFrame(confusion_matrix(total_test_targets, total_test_preds),\n",
    "                              columns=list(CATEGORIES.keys()),\n",
    "                              index=list(CATEGORIES.keys()))\n",
    "\n",
    "test_confusion"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "from random import randrange\n",
    "\n",
    "torch.cuda.empty_cache() # release GPU memory\n",
    "\n",
    "# get random sample from test data \n",
    "random_idx = randrange(len(test_sample_dataset))\n",
    "points, seg = test_sample_dataset.__getitem__(random_idx)\n",
    "\n",
    "# normalize points\n",
    "norm_points = test_sample_dataset.normalize_points(points)\n",
    "\n",
    "with torch.no_grad():\n",
    "    norm_points = norm_points.unsqueeze(0).transpose(2, 1).to(DEVICE)\n",
    "    targets = targets.squeeze().to(DEVICE)\n",
    "\n",
    "    preds, crit_idxs, _ = classifier(norm_points)\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    pred_choice = preds.squeeze().argmax() "
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "source": [
    "pred_class = list(CATEGORIES.keys())[pred_choice.cpu().numpy()]\n",
    "pred_prob = preds[0, pred_choice]\n",
    "print(f'The predicted class is: {pred_class}, with probability: {pred_prob}')"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the probabilities\n",
    "\n",
    "This code was inspired from: https://github.com/intel-isl/Open3D-PointNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "source": [
    "plt.plot(list(CATEGORIES.values()), preds.cpu().numpy()[0]);\n",
    "plt.xticks(list(CATEGORIES.values()), list(CATEGORIES.keys()), rotation=90)\n",
    "plt.title('Predicted Classes')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Probabilities');"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "pcd = o3.geometry.PointCloud()\n",
    "# pcd.points = o3.utility.Vector3dVector(norm_points[0, :, :].cpu().numpy().T)\n",
    "pcd.points = o3.utility.Vector3dVector(points.cpu().numpy())\n",
    "pcd.colors = o3.utility.Vector3dVector(read_pointnet_colors(seg.numpy()))\n",
    "\n",
    "o3.visualization.draw_plotly([pcd])\n",
    "# draw(pcd, point_size=5)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the critical sets\n",
    "\n",
    "The critical sets are the points that make up the basic underlying structure of the point cloud. Now we will see how well the model has learned these.\n",
    "\n",
    "See draw_plotly() source here: https://github.com/isl-org/Open3D/blob/master/python/open3d/visualization/draw_plotly.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "source": [
    "critical_points = points[crit_idxs.squeeze(), :]\n",
    "critical_point_colors = read_pointnet_colors(seg.numpy())[crit_idxs.cpu().squeeze(), :]\n",
    "\n",
    "pcd = o3.geometry.PointCloud()\n",
    "pcd.points = o3.utility.Vector3dVector(critical_points)\n",
    "pcd.colors = o3.utility.Vector3dVector(critical_point_colors)\n",
    "\n",
    "# o3.visualization.draw_plotly([pcd])\n",
    "draw(pcd, point_size=5) # does not work in Colab"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3515861ec4313dacaa20b0eec5bf326e6557b6589b7b6a4fe3c8baa566747d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
