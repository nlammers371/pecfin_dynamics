import os
import zarr
import napari
import numpy as np
from omnipose.core import compute_masks
from cellpose.core import use_gpu
from skimage.segmentation import watershed
from scipy.ndimage import zoom
from tqdm import tqdm
from scipy import stats
import pandas as pd

import torch
root = "E:\\Nick\Cole Trapnell's Lab Dropbox\\Nick Lammers\\Nick\pecfin_dynamics\\fin_morphodynamics\\"
scale_vec = np.asarray([2.0, 0.55, 0.55])
experiment_date = "20240223"
model_name = "log-v5"

# make directory paths
label_directory = os.path.join(root, "built_data", "cellpose_output", model_name, experiment_date, '')
data_directory = os.path.join(root, "built_data", "zarr_image_files", experiment_date, '')
cellpose_directory = os.path.join(root, "built_data", "cellpose_output", model_name, experiment_date, '')

#########
# load the original labels, probs, and grads generated by CellPose
time_int = 171
well_int = 12
file_prefix = experiment_date + f"_well{well_int:04}"
prob_name = os.path.join(cellpose_directory, file_prefix + "_probs.zarr")
grad_name = os.path.join(cellpose_directory, file_prefix + "_grads.zarr")
mask_name = os.path.join(cellpose_directory, file_prefix + "_labels.zarr")

mask_zarr = zarr.open(mask_name, mode="r")
prob_zarr = zarr.open(prob_name, mode="r")
grad_zarr = zarr.open(grad_name, mode="r")

# load the raw image data
data_tzyx = zarr.open(os.path.join(data_directory, file_prefix + ".zarr"), mode="r")

# subset top the desired time point
data_zyx = data_tzyx[time_int, :, :, :]
# viewer = napari.view_image(data_zyx, scale=tuple(scale_vec))

# use affinity graph method from omnipose core to stitch masks at different probability levels

# do the stitching
cp_mask_array = mask_zarr[time_int, :, :, :]
grad_array = grad_zarr[time_int, :, :, :, :]
prob_array = prob_zarr[time_int, :, :, :]
cp_mask_array = cp_mask_array[:, 500:775, 90:435]
grad_array = grad_array[:, :, 500:775, 90:435]
prob_array = prob_array[:, 500:775, 90:435]
# rescale arrays to be isotropic
shape_orig = np.asarray(prob_array.shape)
shape_iso = shape_orig.copy()
iso_factor = scale_vec[0] / scale_vec[1]
shape_iso[0] = int(shape_iso[0] * iso_factor)

niter = 100
min_size = 50
max_size = 1e5
boundary_seg = False
affinity_seg = True
do_3D = True
use_GPU = use_gpu()

device = (
            "cuda"
            if use_GPU
            else "cpu"
        )

print("Resizing arrays...")
zoom_factor = np.divide(shape_iso, shape_orig)
cp_array_rs = zoom(cp_mask_array, zoom_factor, order=0)
grad_array_rs = zoom(grad_array, (1,) + tuple(zoom_factor), order=1)
prob_array_rs = zoom(prob_array, zoom_factor, order=1)   # resize(prob_array, shape_iso, preserve_range=True, order=1)

max_prob = 16
min_prob = -12
prob_increment = 4
mask_thresh_list = list(range(min_prob, max_prob+prob_increment, prob_increment))
seg_hypothesis_array = np.zeros((len(mask_thresh_list), ) + prob_array_rs.shape, dtype=mask_zarr.dtype)

print("Calculating affinity masks...")
for m, mask_threshold in enumerate(tqdm(mask_thresh_list)):
    mask_aff, _, _, _, _ = compute_masks(grad_array_rs, prob_array_rs,
                                                        do_3D=do_3D,
                                                        niter=niter,
                                                        boundary_seg=boundary_seg,
                                                        affinity_seg=affinity_seg,
                                                        min_size=min_size,
                                                        max_size=max_size,
                                                        mask_threshold=mask_threshold,
                                                        interp=True,
                                                        verbose=True,
                                                        omni=True,
                                                        cluster=False,
                                                        use_gpu=use_GPU,
                                                        device=device,
                                                        nclasses=2,
                                                        dim=3)

    seg_hypothesis_array[m] = mask_aff


######
# performing hierarchical watershed

# initialize
masks_curr = seg_hypothesis_array[0]  # start with the most permissive mask

for m in tqdm(range(1, len(mask_thresh_list))):

    # get next layer of labels
    aff_labels = seg_hypothesis_array[m]

    # get union of two masks
    mask_u = (masks_curr + aff_labels) > 0

    # get label vectors
    curr_vec = masks_curr[mask_u]
    next_vec = aff_labels[mask_u]

    # get index vec
    u_indices = np.where(mask_u)
    # linear_indices = np.ravel_multi_index(u_indices, masks_curr.shape)

    # get lists of unique labels
    labels_u_curr = np.unique(curr_vec)
    # labels_u_next = np.unique(next_vec)

    # for each label in the new layer, find label in prev layer that it most overlaps
    # top_label_vec = np.empty(labels_u_next.shape)

    # top_label_frac_vec = np.empty(labels_u_1.shape)
    lb_df = pd.DataFrame(next_vec, columns=["next"])
    lb_df["curr"] = curr_vec
    m_df = lb_df.groupby(by="next").agg(pd.Series.mode).reset_index()
    top_label_vec = m_df.loc[:, "curr"].to_numpy()

    lb_df = lb_df.merge(m_df.rename(columns={"curr": "top_curr"}), how="left", on="next")
    # for i, lb in enumerate(labels_u_next):
    #     top_label_vec[i] = stats.mode(curr_vec[next_vec == lb])[0]

    # initialize mask and marker arrays for watershed
    mask_array = (masks_curr > 0)*1  # this dictates the limits of what can be foreground
    # marker_array = np.zeros(masks_curr.shape, dtype=np.uint16)  # these are the "seeds" for the watershed
    #
    # # add base labels to mask array
    # lb_i = 0
    # for i, lb in enumerate(labels_u_next):
    #     ft = (curr_vec == top_label_vec[i]) & (next_vec == lb)
    #     lb_indices = tuple(u[ft] for u in u_indices)
    #     marker_array[lb_indices] = lb_i   # reset label numbers
    #     lb_i += 1

    marker_array = np.zeros(masks_curr.shape, dtype=np.uint16)
    ft = (lb_df.loc[:, "curr"] == lb_df.loc[:, "top_curr"])
    lb_indices = tuple(u[ft] for u in u_indices)
    marker_array[lb_indices] = next_vec[ft]

    # NL: note for now that we are ignoring next labels that are predominantly in "background" regions.
    # These could be added, but they make up a tiny fraction, and I worry that doing so may lead to
    # fragmented/low-quality masks

    # add markers from base that do not appear in new layer
    included_base_labels = np.unique(top_label_vec)
    max_lb_curr = np.max(marker_array)
    missing_labels = np.asarray(list(set(labels_u_curr) - set(included_base_labels)))

    
    for i, lb in enumerate(missing_labels):
        marker_array[masks_curr == lb] = max_lb_curr + i + 1

    # calculate watershed
    wt_array = watershed(image=-prob_array_rs, markers=marker_array, mask=mask_array, watershed_line=False)

    masks_curr = wt_array
# viewer.add_labels(wt_array)

print("why?")