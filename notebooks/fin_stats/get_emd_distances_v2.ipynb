{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9b5d00-d156-4d69-88e1-c009da523d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from src.utilities.fin_shape_utils import plot_mesh\n",
    "from src.utilities.fin_class_def import FinData\n",
    "from src.utilities.functions import path_leaf\n",
    "import glob2 as glob\n",
    "import trimesh\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import geomloss\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5646db-7719-4e3b-97b0-1b42598809fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of refined fin mesh objects\n",
    "root = \"/media/nick/hdd02/Cole Trapnell's Lab Dropbox/Nick Lammers/Nick/pecfin_dynamics/\"\n",
    "fin_mesh_list = sorted(glob.glob(os.path.join(root, \"point_cloud_data\", \"processed_fin_data\", \"*smoothed_fin_mesh*\")))\n",
    "\n",
    "# load metadata\n",
    "metadata_df = pd.read_csv(os.path.join(root, \"metadata\", \"master_metadata.csv\"))\n",
    "metadata_df[\"experiment_date\"] = metadata_df[\"experiment_date\"].astype(str)\n",
    "metadata_df.head()\n",
    "\n",
    "# set write directory\n",
    "write_dir = os.path.join(root, \"point_cloud\", \"fin_shape_analyses\")\n",
    "if not os.path.isdir(write_dir):\n",
    "    os.makedirs(write_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea9860-fc44-4ef6-b03f-5c88431e4609",
   "metadata": {},
   "source": [
    "### Calculate approximate Wassersten distance between:\n",
    "1) Mesh surface points \n",
    "2) Fin centroids\n",
    "3) Mesh surface points (normalized case)\n",
    "4) Mesh surface points (normalized case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be8223c-3113-4fdc-8ba0-0b0c0f36bd0a",
   "metadata": {},
   "source": [
    "### Preload data structures for distance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "26b6633e-bdd3-442f-b853-f80bf5cf3e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [00:20<00:00, 11.15it/s]\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "surf_list = []\n",
    "surf_norm_list = []\n",
    "centroid_list = []\n",
    "centroid_norm_list = []\n",
    "\n",
    "n_points = 250\n",
    "np.random.seed(125)\n",
    "# preload meshes and generate metadata table\n",
    "\n",
    "for file_ind0, fp0 in enumerate(tqdm(fin_mesh_list)):\n",
    "\n",
    "    fp0_centroid = fp0.replace(\"smoothed_fin_mesh.obj\", \"fin_data_upsampled.csv\")\n",
    "    # extract relevant metadata\n",
    "    fname0 = os.path.basename(fp0)\n",
    "    well_ind0 = fname0.find(\"well\")\n",
    "    date_string0 = fname0[:well_ind0-1]\n",
    "    well_num0 = int(fname0[well_ind0+4:well_ind0+8])\n",
    "    time_ind0 = fname0.find(\"time\")\n",
    "    time_num0 = int(fname0[time_ind0+4:time_ind0+8])\n",
    "    \n",
    "    # match this to a row in the metadata df\n",
    "    date_ft0 = metadata_df[\"experiment_date\"] == date_string0\n",
    "    well_ft0 = metadata_df[\"well_index\"] == well_num0\n",
    "    time_ft0 = metadata_df[\"time_index\"] == time_num0\n",
    "    \n",
    "    meta_temp = metadata_df.loc[date_ft0 & well_ft0 & time_ft0, :].reset_index(drop=True)\n",
    "    \n",
    "    cv = meta_temp.loc[0, \"chem_i\"]\n",
    "    if isinstance(cv, str):\n",
    "        cvs = cv.split(\"_\")\n",
    "        chem_id = cvs[0]\n",
    "        chem_time = int(cvs[1])\n",
    "    else:\n",
    "        chem_id = \"WT\"\n",
    "        chem_time = np.nan\n",
    "    meta_temp.loc[0, \"chem_id\"] = chem_id\n",
    "    meta_temp.loc[0, \"chem_time\"] = chem_time\n",
    "\n",
    "    # load mesh\n",
    "    fin_mesh = trimesh.load(fp0)\n",
    "    fin_df = pd.read_csv(fp0_centroid)\n",
    "\n",
    "    fin_df_c = fin_df.loc[:, [\"nucleus_id\", \"XB\", \"YB\", \"ZB\"]].groupby(\"nucleus_id\").mean().reset_index(drop=False)\n",
    "    options = np.arange(fin_df_c.shape[0])\n",
    "    c_to_samp = np.random.choice(options, n_points, replace=True)\n",
    "\n",
    "    c = torch.tensor(fin_df_c[[\"XB\", \"YB\", \"ZB\"]].to_numpy())\n",
    "    c = c[c_to_samp, :]\n",
    "    cn = c.clone() - torch.mean(c, axis=0)\n",
    "    mn = np.amax(norm(c, axis=1))\n",
    "    cn /= mn\n",
    "    \n",
    "    # convert to tensor format\n",
    "    s = torch.tensor(fin_mesh.sample(n_points))\n",
    "    sn = s.clone() - torch.mean(s, axis=0)\n",
    "    mn = np.amax(norm(s, axis=1))\n",
    "    sn /= mn\n",
    "\n",
    "    if sn.shape[0] > 0:\n",
    "        df_list.append(meta_temp)\n",
    "        \n",
    "        # surf points\n",
    "        surf_list.append(s)\n",
    "        surf_norm_list.append(sn)\n",
    "\n",
    "        # centroids\n",
    "        centroid_list.append(c)\n",
    "        centroid_norm_list.append(cn)\n",
    "\n",
    "dist_df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36df40c1-1314-4cb6-9870-4c7bce243b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb8e81b6-5e4b-4661-b3f7-b03e8a590a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224/224 [11:48<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "# calculate distances\n",
    "\n",
    "# Create a Sinkhorn loss instance\n",
    "loss = geomloss.SamplesLoss(\"sinkhorn\", p=1, blur=0.5)\n",
    "\n",
    "# initialize array to store distance\n",
    "dist_arr0 = np.empty((len(fin_mesh_list), len(fin_mesh_list)))\n",
    "distance_array[:] = np.nan\n",
    "dist_arr1 = dist_arr0.copy()\n",
    "dist_arr2 = dist_arr0.copy()\n",
    "dist_arr3 = dist_arr0.copy()\n",
    "\n",
    "# iterate\n",
    "for i in tqdm(range(len(surf_list))):\n",
    "    si = surf_list[i]\n",
    "    sni = surf_norm_list[i]\n",
    "    ci = centroid_list[i]\n",
    "    cni = centroid_norm_list[i]\n",
    "    \n",
    "    for j in (range(i+1, len(surf_list))):\n",
    "        sj = surf_list[j]\n",
    "        snj = surf_norm_list[j]\n",
    "        cj = centroid_list[j]\n",
    "        cnj = centroid_norm_list[j]\n",
    "        \n",
    "        # (1) Compute distance between surf points\n",
    "        dist0 = loss(si, sj)\n",
    "    \n",
    "        dist_arr0[i, j] = dist0\n",
    "        dist_arr0[j, i] = dist0\n",
    "        \n",
    "        # (2) Compute distance between centroid points\n",
    "        dist1 = loss(ci, cj)\n",
    "    \n",
    "        dist_arr1[i, j] = dist1\n",
    "        dist_arr1[j, i] = dist1\n",
    "        \n",
    "        # (3) Compute distance between NORMALIZED surf points\n",
    "        dist2 = loss(sni, snj)\n",
    "    \n",
    "        dist_arr2[i, j] = dist2\n",
    "        dist_arr2[j, i] = dist2\n",
    "\n",
    "        # (4) Compute distance between NORMALIZED centroid points\n",
    "        dist3 = loss(cni, cnj)\n",
    "    \n",
    "        dist_arr3[i, j] = dist3\n",
    "        dist_arr3[j, i] = dist3\n",
    "\n",
    "        if (dist0 < 0) |  (dist1 < 0) | (dist2 < 0) | (dist3 < 0):\n",
    "            print(f\"why_{i}_{j}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1952cba8-68b7-411b-a6a2-f86bb26ddce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([53400, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21e6499b-37c6-4916-8a91-4e5df141d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort array and DF for clarity\n",
    "indices = np.lexsort((dist_df['chem_id'], dist_df['chem_time']))\n",
    "dist_df_s = dist_df.iloc[indices]\n",
    "\n",
    "dist_arr0_s = dist_arr0.copy()\n",
    "dist_arr0_s = dist_arr0_s[indices, :]\n",
    "dist_arr0_s = dist_arr0_s[:, indices]\n",
    "\n",
    "dist_arr1_s = dist_arr1.copy()\n",
    "dist_arr1_s = dist_arr1_s[indices, :]\n",
    "dist_arr1_s = dist_arr1_s[:, indices]\n",
    "\n",
    "dist_arr2_s = dist_arr2.copy()\n",
    "dist_arr2_s = dist_arr2_s[indices, :]\n",
    "dist_arr2_s = dist_arr2_s[:, indices]\n",
    "\n",
    "dist_arr3_s = dist_arr3.copy()\n",
    "dist_arr3_s = dist_arr3_s[indices, :]\n",
    "dist_arr3_s = dist_arr3_s[:, indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d32abc9b-15d8-48ff-a5c5-2b40a307c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \n",
    "dist_df_s.to_csv(os.path.join(write_dir, \"emd_dist_df.csv\"), index=False)\n",
    "\n",
    "np.save(os.path.join(write_dir, \"surf_dist_arr.npy\"), dist_arr0_s)\n",
    "np.save(os.path.join(write_dir, \"centroid_dist_arr.npy\"), dist_arr1_s)\n",
    "np.save(os.path.join(write_dir, \"surf_dist_norm_arr.npy\"), dist_arr2_s)\n",
    "np.save(os.path.join(write_dir, \"centroid_dist_norm_arr.npy\"), dist_arr3_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b977cd8e-f684-496c-9813-d797b43b9c42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
